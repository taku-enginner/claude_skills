#!/usr/bin/env python3
"""
Error/Anomaly Screen Detection Skill for Claude Code

å‹•ç”»ã‚„ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ã‚¨ãƒ©ãƒ¼ç”»é¢ã‚„ç•°å¸¸ãªç”»é¢ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã™ã€‚
- èµ¤ã„ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
- ãƒ€ã‚¤ã‚¢ãƒ­ã‚°/ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—
- ç©ºç™½ç”»é¢
- ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ç”»é¢
"""

import argparse
import os
import sys
from pathlib import Path
from typing import List, Dict

try:
    import cv2
    import numpy as np
    from PIL import Image
except ImportError:
    print("å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("  pip install opencv-python numpy Pillow")
    sys.exit(1)


class AnomalyDetector:
    """ç•°å¸¸ç”»é¢ã‚’æ¤œå‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã§ã‚ˆãä½¿ã‚ã‚Œã‚‹èµ¤è‰²ã®ç¯„å›² (HSV)
        self.error_red_lower = np.array([0, 100, 100])
        self.error_red_upper = np.array([10, 255, 255])
        self.error_red_lower2 = np.array([160, 100, 100])
        self.error_red_upper2 = np.array([180, 255, 255])

        # è­¦å‘Šã§ã‚ˆãä½¿ã‚ã‚Œã‚‹é»„è‰²/ã‚ªãƒ¬ãƒ³ã‚¸ã®ç¯„å›² (HSV)
        self.warning_lower = np.array([15, 100, 100])
        self.warning_upper = np.array([35, 255, 255])

    def detect_red_error(self, frame: np.ndarray) -> Dict:
        """èµ¤ã„ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã‚’æ¤œå‡º"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

        # èµ¤è‰²ãƒã‚¹ã‚¯ï¼ˆ2ã¤ã®ç¯„å›²ã‚’çµåˆï¼‰
        mask1 = cv2.inRange(hsv, self.error_red_lower, self.error_red_upper)
        mask2 = cv2.inRange(hsv, self.error_red_lower2, self.error_red_upper2)
        red_mask = cv2.bitwise_or(mask1, mask2)

        # èµ¤è‰²ã®å‰²åˆã‚’è¨ˆç®—
        red_ratio = np.count_nonzero(red_mask) / red_mask.size

        # é€£ç¶šã—ãŸèµ¤ã„é ˜åŸŸã‚’æ¤œå‡º
        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        large_red_regions = [c for c in contours if cv2.contourArea(c) > 500]

        return {
            "detected": red_ratio > 0.01 or len(large_red_regions) > 0,
            "red_ratio": round(red_ratio * 100, 2),
            "red_region_count": len(large_red_regions),
            "type": "error_red"
        }

    def detect_warning(self, frame: np.ndarray) -> Dict:
        """è­¦å‘Šè¡¨ç¤ºï¼ˆé»„è‰²/ã‚ªãƒ¬ãƒ³ã‚¸ï¼‰ã‚’æ¤œå‡º"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        warning_mask = cv2.inRange(hsv, self.warning_lower, self.warning_upper)

        warning_ratio = np.count_nonzero(warning_mask) / warning_mask.size

        contours, _ = cv2.findContours(warning_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        large_warning_regions = [c for c in contours if cv2.contourArea(c) > 500]

        return {
            "detected": warning_ratio > 0.02 or len(large_warning_regions) > 0,
            "warning_ratio": round(warning_ratio * 100, 2),
            "warning_region_count": len(large_warning_regions),
            "type": "warning"
        }

    def detect_blank_screen(self, frame: np.ndarray) -> Dict:
        """ç©ºç™½/ç™½ç”»é¢ã‚’æ¤œå‡º"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # ç™½ã«è¿‘ã„ãƒ”ã‚¯ã‚»ãƒ«ã®å‰²åˆ
        white_mask = gray > 240
        white_ratio = np.count_nonzero(white_mask) / white_mask.size

        # é»’ã«è¿‘ã„ãƒ”ã‚¯ã‚»ãƒ«ã®å‰²åˆ
        black_mask = gray < 15
        black_ratio = np.count_nonzero(black_mask) / black_mask.size

        # å˜è‰²ã«è¿‘ã„ã‹ãƒã‚§ãƒƒã‚¯
        std_dev = np.std(gray)

        is_blank = (white_ratio > 0.9 or black_ratio > 0.9) and std_dev < 20

        return {
            "detected": is_blank,
            "white_ratio": round(white_ratio * 100, 2),
            "black_ratio": round(black_ratio * 100, 2),
            "uniformity": round(std_dev, 2),
            "type": "blank_screen"
        }

    def detect_dialog(self, frame: np.ndarray) -> Dict:
        """ãƒ€ã‚¤ã‚¢ãƒ­ã‚°/ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’æ¤œå‡º"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # ã‚¨ãƒƒã‚¸æ¤œå‡º
        edges = cv2.Canny(gray, 50, 150)

        # çŸ©å½¢ã‚’æ¤œå‡º
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        h, w = frame.shape[:2]
        center_x, center_y = w // 2, h // 2

        dialog_candidates = []
        for contour in contours:
            x, y, cw, ch = cv2.boundingRect(contour)
            area = cw * ch
            aspect_ratio = cw / ch if ch > 0 else 0

            # ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®ç‰¹å¾´ã‚’ãƒã‚§ãƒƒã‚¯
            # - ç”»é¢ã®ä¸­å¤®ä»˜è¿‘ã«ã‚ã‚‹
            # - é©åº¦ãªã‚µã‚¤ã‚ºï¼ˆç”»é¢ã®10-80%ï¼‰
            # - æ¨ªé•·ã¾ãŸã¯æ­£æ–¹å½¢ã«è¿‘ã„
            is_centered = (abs(x + cw/2 - center_x) < w * 0.3 and
                          abs(y + ch/2 - center_y) < h * 0.3)
            is_dialog_size = 0.05 < area / (w * h) < 0.8
            is_dialog_shape = 0.5 < aspect_ratio < 3.0

            if is_centered and is_dialog_size and is_dialog_shape:
                dialog_candidates.append({
                    "x": x, "y": y, "width": cw, "height": ch
                })

        return {
            "detected": len(dialog_candidates) > 0,
            "dialog_count": len(dialog_candidates),
            "dialogs": dialog_candidates[:3],  # æœ€å¤§3ã¤ã¾ã§
            "type": "dialog"
        }

    def detect_loading(self, frame: np.ndarray) -> Dict:
        """ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”»é¢ã‚’æ¤œå‡ºï¼ˆå††å½¢ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãªã©ï¼‰"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # å††ã‚’æ¤œå‡º
        circles = cv2.HoughCircles(
            gray, cv2.HOUGH_GRADIENT, 1, 50,
            param1=100, param2=30, minRadius=20, maxRadius=100
        )

        # ç”»é¢ã®ä¸­å¤®ä»˜è¿‘ã«å††ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        h, w = frame.shape[:2]
        center_x, center_y = w // 2, h // 2

        loading_indicators = []
        if circles is not None:
            for circle in circles[0]:
                cx, cy, r = circle
                # ä¸­å¤®ä»˜è¿‘ã«ã‚ã‚‹ã‹
                if abs(cx - center_x) < w * 0.3 and abs(cy - center_y) < h * 0.3:
                    loading_indicators.append({
                        "x": int(cx), "y": int(cy), "radius": int(r)
                    })

        return {
            "detected": len(loading_indicators) > 0,
            "indicator_count": len(loading_indicators),
            "type": "loading"
        }

    def analyze_frame(self, frame: np.ndarray) -> Dict:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç·åˆçš„ã«åˆ†æ"""
        results = {
            "error_red": self.detect_red_error(frame),
            "warning": self.detect_warning(frame),
            "blank_screen": self.detect_blank_screen(frame),
            "dialog": self.detect_dialog(frame),
            "loading": self.detect_loading(frame),
        }

        # æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
        anomalies = []
        severity = "normal"

        if results["error_red"]["detected"]:
            anomalies.append("ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºï¼ˆèµ¤ï¼‰")
            severity = "error"
        if results["blank_screen"]["detected"]:
            anomalies.append("ç©ºç™½ç”»é¢")
            severity = "error" if severity != "error" else severity
        if results["warning"]["detected"]:
            anomalies.append("è­¦å‘Šè¡¨ç¤º")
            if severity == "normal":
                severity = "warning"
        if results["dialog"]["detected"]:
            anomalies.append("ãƒ€ã‚¤ã‚¢ãƒ­ã‚°/ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—")
            if severity == "normal":
                severity = "info"
        if results["loading"]["detected"]:
            anomalies.append("ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
            if severity == "normal":
                severity = "info"

        results["summary"] = {
            "has_anomaly": len(anomalies) > 0,
            "anomalies": anomalies,
            "severity": severity,
        }

        return results


def process_video(
    video_path: str,
    output_dir: str,
    sample_interval: int = 10,
    jpeg_quality: int = 50,
    scale: float = 0.5,
) -> Dict:
    """å‹•ç”»ã‹ã‚‰ç•°å¸¸ç”»é¢ã‚’æ¤œå‡º"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"å‹•ç”»ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    detector = AnomalyDetector()
    anomaly_frames = []
    frame_idx = 0

    print(f"å‹•ç”»ã‚’åˆ†æä¸­: {video_path}")
    print(f"  ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames}, FPS: {fps:.2f}")
    print(f"  ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”: {sample_interval}ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % sample_interval == 0:
            results = detector.analyze_frame(frame)

            if results["summary"]["has_anomaly"]:
                timestamp = frame_idx / fps if fps > 0 else 0

                # ç”»åƒã‚’ä¿å­˜
                if scale != 1.0:
                    h, w = frame.shape[:2]
                    frame = cv2.resize(frame, (int(w * scale), int(h * scale)))

                filename = f"anomaly_{frame_idx:06d}_{results['summary']['severity']}.jpg"
                filepath = output_path / filename
                cv2.imwrite(str(filepath), frame, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality])

                anomaly_frames.append({
                    "frame_idx": frame_idx,
                    "timestamp": round(timestamp, 2),
                    "timestamp_str": f"{int(timestamp // 60)}:{timestamp % 60:05.2f}",
                    "severity": results["summary"]["severity"],
                    "anomalies": results["summary"]["anomalies"],
                    "file": str(filepath),
                    "details": {k: v for k, v in results.items() if k != "summary"}
                })

        frame_idx += 1

    cap.release()

    return {
        "video_path": video_path,
        "total_frames": total_frames,
        "analyzed_frames": total_frames // sample_interval,
        "anomaly_count": len(anomaly_frames),
        "anomaly_frames": anomaly_frames,
        "output_dir": str(output_path),
    }


def process_images(
    image_paths: List[str],
    output_dir: str,
    jpeg_quality: int = 50,
    scale: float = 0.5,
) -> Dict:
    """è¤‡æ•°ã®ç”»åƒã‹ã‚‰ç•°å¸¸ç”»é¢ã‚’æ¤œå‡º"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    detector = AnomalyDetector()
    anomaly_images = []

    print(f"ç”»åƒã‚’åˆ†æä¸­: {len(image_paths)}æš")

    for i, image_path in enumerate(image_paths):
        frame = cv2.imread(image_path)
        if frame is None:
            print(f"  è­¦å‘Š: èª­ã¿è¾¼ã‚ã¾ã›ã‚“: {image_path}")
            continue

        results = detector.analyze_frame(frame)

        if results["summary"]["has_anomaly"]:
            # ç”»åƒã‚’ä¿å­˜
            if scale != 1.0:
                h, w = frame.shape[:2]
                frame = cv2.resize(frame, (int(w * scale), int(h * scale)))

            filename = f"anomaly_{i:04d}_{results['summary']['severity']}.jpg"
            filepath = output_path / filename
            cv2.imwrite(str(filepath), frame, [cv2.IMWRITE_JPEG_QUALITY, jpeg_quality])

            anomaly_images.append({
                "original_file": image_path,
                "severity": results["summary"]["severity"],
                "anomalies": results["summary"]["anomalies"],
                "output_file": str(filepath),
                "details": {k: v for k, v in results.items() if k != "summary"}
            })

    return {
        "total_images": len(image_paths),
        "anomaly_count": len(anomaly_images),
        "anomaly_images": anomaly_images,
        "output_dir": str(output_path),
    }


def print_video_result(result: Dict):
    """å‹•ç”»åˆ†æçµæœã‚’è¡¨ç¤º"""
    print("\n" + "=" * 50)
    print("åˆ†æå®Œäº†!")
    print("=" * 50)

    print(f"\nåˆ†æå¯¾è±¡: {result['video_path']}")
    print(f"åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {result['analyzed_frames']}")
    print(f"ç•°å¸¸æ¤œå‡ºæ•°: {result['anomaly_count']}ä»¶")

    if result["anomaly_frames"]:
        # æ·±åˆ»åº¦åˆ¥ã«é›†è¨ˆ
        by_severity = {}
        for af in result["anomaly_frames"]:
            sev = af["severity"]
            by_severity[sev] = by_severity.get(sev, 0) + 1

        print(f"\næ·±åˆ»åº¦åˆ¥:")
        for sev, count in sorted(by_severity.items()):
            icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(sev, "âšª")
            print(f"  {icon} {sev}: {count}ä»¶")

        print(f"\næ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ç”»é¢:")
        for af in result["anomaly_frames"][:10]:
            icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(af["severity"], "âšª")
            anomalies_str = ", ".join(af["anomalies"])
            print(f"  {icon} [{af['timestamp_str']}] {anomalies_str}")
            print(f"     â†’ {af['file']}")

        if len(result["anomaly_frames"]) > 10:
            print(f"  ... ä»– {len(result['anomaly_frames']) - 10} ä»¶")

    print(f"\nå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {result['output_dir']}")


def print_image_result(result: Dict):
    """ç”»åƒåˆ†æçµæœã‚’è¡¨ç¤º"""
    print("\n" + "=" * 50)
    print("åˆ†æå®Œäº†!")
    print("=" * 50)

    print(f"\nåˆ†æç”»åƒæ•°: {result['total_images']}")
    print(f"ç•°å¸¸æ¤œå‡ºæ•°: {result['anomaly_count']}ä»¶")

    if result["anomaly_images"]:
        print(f"\næ¤œå‡ºã•ã‚ŒãŸç•°å¸¸ç”»é¢:")
        for ai in result["anomaly_images"]:
            icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(ai["severity"], "âšª")
            anomalies_str = ", ".join(ai["anomalies"])
            print(f"  {icon} {ai['original_file']}")
            print(f"     {anomalies_str}")
            print(f"     â†’ {ai['output_file']}")

    print(f"\nå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {result['output_dir']}")


def main():
    parser = argparse.ArgumentParser(
        description="å‹•ç”»/ç”»åƒã‹ã‚‰ã‚¨ãƒ©ãƒ¼ç”»é¢ã‚„ç•°å¸¸ãªç”»é¢ã‚’è‡ªå‹•æ¤œå‡º",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å‹•ç”»ã‹ã‚‰ç•°å¸¸ç”»é¢ã‚’æ¤œå‡º
  python detect_anomaly_screens.py video.mp4

  # è¤‡æ•°ã®ç”»åƒã‹ã‚‰æ¤œå‡º
  python detect_anomaly_screens.py screenshot1.png screenshot2.png screenshot3.png

  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ã
  python detect_anomaly_screens.py video.mp4 -o anomalies -i 5 -s 0.3

æ¤œå‡ºå¯¾è±¡:
  - ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºï¼ˆèµ¤ã„è­¦å‘Šãªã©ï¼‰
  - è­¦å‘Šè¡¨ç¤ºï¼ˆé»„è‰²/ã‚ªãƒ¬ãƒ³ã‚¸ï¼‰
  - ç©ºç™½/ç™½ç”»é¢
  - ãƒ€ã‚¤ã‚¢ãƒ­ã‚°/ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—
  - ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”»é¢
        """
    )
    parser.add_argument("inputs", nargs="+", help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("-o", "--output", default="anomaly_output", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: anomaly_output)")
    parser.add_argument("-i", "--interval", type=int, default=10, help="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ï¼ˆå‹•ç”»ã®ã¿ã€default: 10ï¼‰")
    parser.add_argument("-s", "--scale", type=float, default=0.5, help="å‡ºåŠ›ç”»åƒã®ã‚¹ã‚±ãƒ¼ãƒ« (default: 0.5)")
    parser.add_argument("-q", "--quality", type=int, default=50, help="JPEGå“è³ª (default: 50)")

    args = parser.parse_args()

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    for path in args.inputs:
        if not os.path.exists(path):
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}")
            sys.exit(1)

    try:
        # å‹•ç”»ã‹ç”»åƒã‹ã‚’åˆ¤å®š
        video_extensions = {".mp4", ".avi", ".mov", ".mkv", ".webm"}
        first_ext = Path(args.inputs[0]).suffix.lower()

        if first_ext in video_extensions:
            # å‹•ç”»ã¨ã—ã¦å‡¦ç†
            result = process_video(
                video_path=args.inputs[0],
                output_dir=args.output,
                sample_interval=args.interval,
                jpeg_quality=args.quality,
                scale=args.scale,
            )
            print_video_result(result)
        else:
            # ç”»åƒã¨ã—ã¦å‡¦ç†
            result = process_images(
                image_paths=args.inputs,
                output_dir=args.output,
                jpeg_quality=args.quality,
                scale=args.scale,
            )
            print_image_result(result)

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
